# Microdata

The Metadata Editor makes use of the Data Documentation Initiative (DDI) metadata standard for the documentation of microdata. The DDI version implemented in the Metadata Editor is the DDI Codebook, version 2.5.

## The DDI Codebook metadata standard

The DDI Codebook metadata standard is developed and maintained by the DDI Alliance. It contains the following main sections:
- **Document description**: contains a small number of elements used to document the metadata (not the data) 
- **Study description**: contains all elements related to document the study (survey, census or other) itself, including the title, producers, geographic and temporal coverage, sampling, etc.
- **File description**: contains a few elements used to provide information on each data file composing the dataset.
- **Variable description**: contains elements used to describe in detail each variable in the dataset. This includes variable names and labels, value labels, literal questions and interviewer instructions, summary statistics, and more. This section of the DDI Codebook provides a detailed data dictionary.
- **Variable groups**: contains elements to organize the variables by groups (other than the data file they belong to). This section is optional.

For the documentation of microdata, the Metadata Editor also makes use of the Dublin Core metadata standard to document **external resources**. External resources are files or links that provide content other than the metadata stored in the DDI. This may consist of PDF questionnaires or manuals, scripts, images, or any other resource available in digital format.

## Preparing your data and documentation

The Metadata Editor is not a data editor. Before you start the documentation of your dataset, it is highly recommended to organize your files and properly check your data. We provide here a few suggestions and recommendations. The quality of the output generated by the Metadata Editor will depend upon this prior preparatory and verification work. 

### Gathering and organizing the files

**Organize your files in folders**

Documentation of a dataset will be most efficient if you organize your data and other files properly. We recommend that, before anything else, you create the necessary directories as follows:
![image](https://user-images.githubusercontent.com/35276300/215342948-57ad26a3-db23-435c-aa4c-73a6a2867539.png)

Note
Be careful not to overwrite your original variables. Since managing databases involves several data-checking procedures, archive a new version in addition to the original. Work on this new version, leaving the original data files untouched.

### Run quality checks

Prior to documenting a dataset, it is important to ensure that you are working with the most appropriate version of all the concerned data files. If the dataset is meant for public release, you should work with the final, edited, anonymous version of the dataset. If the dataset is being documented for archiving and internal use only, one may include the raw data as well as the final, fully edited files. The Metadata Editor provides you with the possibility of documenting the specificity of each version of the dataset.

This section describes various checks and balances involved in the data preparation process. Listed below are some common data problems that users encounter:

- Absence of variables that uniquely identify each record of the dataset
- Duplicate observations
- Errors from merging multiple datasets
- Encountering incomplete data when comparing the content of the data files with the original survey questionnaire
- Unlabelled data
- Variables with missing values
- Unnecessary or temporary variables in the data files
- Data with sensitive information or direct identifiers
- Some practical examples using a statistical package are provided in Section A “Data Validations in Stata: Practical Examples.

The following procedures are recommended for preparing your dataset(s):

1. Organized your data in a hierarchical format 

It is preferable to organize your files in a hierarchical format instead of a flat format. In a hierarchical format, columns contain specific information about all possible units of analysis and rows form the individual observations (households, establishments, products, communities/countries, or any combination of those). Hierarchical files are easier to analyse, as they contain fewer columns that store the same information and are more compact. A flat format contains multiple columns with information on only one specific unit of analysis, so the information becomes redundant. For example, the information provided in one column is about the household head, and the row provides information on the child in the household. Hierarchical files are easier to manage. Suppose in this example that there were many characteristics measured for everyone, the hierarchical structure would be a more convenient format because for each new characteristic, the dataset creates only one additional column, whereas, in the flat structure, it would create as many columns as there are people in the data with such characteristics.

2. Data with multiple units of analysis should be stored in different data files

It is recommended that you store your data in different files when you have multiple observational units. For example, Table 3 shows a dataset that has both household-level data (columns on the type of dwelling and walls material) and individual-level data (columns on the marital status, work status, and worker category). Note that storing both levels of information in one dataset will result in a repetition of household characteristics for each household member. In Table 3, the information about the columns ‘type of dwelling’ and ‘wall material’ is repeated for everyone. Sometimes, this duplication is inefficient, and it is easier to have the dataset broken down by observational unit, into multiple files. In this example, it would be simpler to create two files: one for the household characteristics and another for the individual characteristics. The two files can be connected through a unique identifier, which in this case will be the household ID and member ID. We discuss the need for this unique identifier further on in this text as well.

3. Unique record identifiers

The absence of a unique identifier is a data quality issue, so one needs to ensure that the unique IDs remain fixed/present during the data cleaning process. If this correction is not possible, the archivist should note the anomalies in the documentation process. 
It is recommended that ID variables be defined as a numeric since sorting and filtering records is much more efficient when variables are numeric. 
ID variables should not contain spaces, special characters or accents, since they may suffer modifications when the dataset is converted in different formats.
For the convenience of users of the data, avoid identifiers consisting of too many variables. For example, in a household survey, the household identifier should ideally be a single variable (which you may create by concatenating a group of variables [3]), and the individual identifier should be the combination of only two variables (the household ID, and the sequential number of each member).
It is recommended that you generate an ID based on a sequential number, however, keep in mind that it should not be too long because statistical packages and spreadsheet programs store a number of digits of precision, so opening a data set that contains ID variables with many characters, might result in truncated fields. For instance, the limit of the number of characters in Microsoft Excel is 15, so it changes any digits past the fifteenth place to zeroes.
If you prepare your data files for public dissemination, it may be preferable to generate a unique household identification that would not be a compilation of geographic codes (because geographic codes are highly identifying). This recommendation is to ensure anonymity and will be explained in further detail later on in this text. The following example shows how to construct a unique identifier without using detailed information provided by the geographic codes.

Even if the data set has a variable with a label “unique identifier”, it is important to confirm that this variable truly does uniquely identify each record. To confirm or even to find out what the unique identifier is, you can make use of the -duplicate- function in SPSS or the -isid- command in Stata (for R, do as shown in Table 6). For more details, refer to Example 1 and Example 2 of Section A. 

Finally, check that the ID variable for the unit of observation doesn’t have missing or assigned zero/null values. Ensure that the datasets are sorted and arranged by their unique identifiers.

4. Check relationships

Use statistical software to validate that all files can be combined into one. For a household survey, for example, verify that all records in the individual-level files have a corresponding household in the household-level master file. Also, verify that all households have at least on e corresponding record in the household-roster file that lists all individuals. Below, some considerations to keep in mind before merging data files:

5. The variable name of the identifier should be the same across all datasets.

The ID variables need to be the same type (either both numeric or both string) across all databases. Except for ID variables, it is highly recommended that the databases don’t share the same variable names or labels.

6. Panel datasets should be stored in different files as well. Having one file per data collection period is a good practice. To combine the different periods of a panel dataset, the data user could merge them (Adding variables to the existing observations for the same period) or append them (Adding observations for a different period to the existing variables). To make sure that panels can be properly appended, the following checks are suggested:

7. Check for missing values

Getting data ready for documentation also involves checking for variables that do not provide complete information because they are full of missing values. This step is important because missing values can have unexpected effects on the data analysis process. Typically, missing values are defined as a character (.a, .b, single period or asterisks), special numeric (-1, -2) or blanks. Variables entirely comprised of missing values should ideally not be included in the dataset. However, before excluding them, it is useful to check whether the missing values are expected according to the questionnaire, and the skip patterns.

8. Check value ranges
It is helpful to generate descriptive statistics for all variables (frequencies for discrete variables; min/max/mean for continuous variables) and verify that these statistics look reasonable. Just as there are variables that must take on only specific values, such as “F” and “M” for gender, there are also some variables that can take on several values (such as age or height). However, those values must fit a particular range. For example, we don’t expect negative values, or typically see values over 115 years for age.

Values for categorical variables should be guided by the questionnaire (or separate documentation for constructed variables). If we have an education variable that has 9 response options in the questionnaire, the corresponding ‘education’ variable in the dataset should have 9 categories. We should not observe more than 9 unique values for this variable. Similarly, for any questions in the survey for which the options are only “yes”, “no” and “other”, we should not observe more than these 3 unique values. When out of range values exist, this might signal data cleaning issues.

9. Verify that the number of records in each file corresponds to what is expected

The technical documentation helps to form some expectations about the size of the dataset. Make sure that in all the files, the number of records is the same as (or is similar to) what is explicitly stated in the sample design of your survey.
Even if the number of individual records is not available in the documentation, you can still perform a rough check on the files. 

10. Check completeness
Datasets must contain all variables from the questionnaire and be in a logic sequence
Verify the completeness of your data files by comparing the content of these files with the survey questionnaire. All variables in the questionnaire should appear in the dataset, except those excluded on purpose by the producer of the data because of reasons of confidentiality (see numeral 1.15). Cross-checking with the questionnaire(s) is needed to ensure that all sections are included in the dataset.
Additionally, it is a good practice to make sure that the database is sorted in the same order as the questionnaire. This practice will help users navigate seamlessly across the dataset using the questionnaire as a route map.

11. Sample weights
Include the relevant weighting coefficients and variables identifying the stratification levels. All data files of a sample survey should have clearly labelled variable(s) with information on the survey weights. Sample surveys need to be representative of a broader population for which the data is collected, and the user needs the survey weights for almost every analysis performed. In the case of household surveys, the survey weights are equal among members of the same household but differ across households. Weights are positive and strictly higher than zero. They should not have a larger value than the population for which the survey is representative.

A more detailed description of how the survey weights would look like should be provided in the documentation of the survey. Based on it, you can perform some basic range checks. Notice that Census datasets do not need weights since a census collects data on all the individuals in the population. There are however some exceptions, for example in the case of IPUMS, the data collected are not full censuses but census samples, so weights are required in this context.

Additionally, for sample surveys, verify that the variables identifying the various levels of stratification and the primary sampling unit are included and easily identifiable in at least one of the data files. These variables are needed for the calculation of sampling errors.

12. Variable labels
Labels should be short and precise. They should provide a clear indication of what information is contained in the variables. Variable labels are brief descriptions or attributes of each variable. Without variable labels, users are not able to link the variables in the database to the questions of the questionnaire. So, one should ensure that all variables are labelled.
Additionally, even if variables are fully labelled, the following practices must be considered:
Variable labels can be up to 80 characters long in Stata and 255 in SPSS, however, it is recommended that labels be informative, short and accurate.
It is a common practice to have a literal question from the survey as a variable label. However, the literal questions are usually longer than the maximum number of characters, so this is not an advisable practice.
The same label should not be used for two different variables.

13. Value labels
Label values are used for categorical variables. To ensure the correct encoding of data, it is important to check that the stored values in those variables correspond to what is expected according to the questionnaire. In the case of continuous variables, we also suggest the checking of ranges. For instance, if the question is about the number of working hours, the variable should not have negative values.

You can compare variable labels in the dataset to those in the questionnaire using the –codebook- Stata command or –labelbook-. Refer to Example 8 of Section A for further details.

14. Non-relevant variables
Temporary, calculated or derived variables should not be disseminated
Remove all unnecessary or temporary variables from the data files. These variables are not collected in the field and present no interest for users.

The data producer could generate variables that are only needed during the quality control process but are not relevant to the final data user. 
There are cases in which calculated variables may be useful to the users, so they must be documented in the metadata. For example, most Labor Force Surveys (LFS) contain derived dummy variables to identify the sections of the population that are employed or unemployed. These variables are generated using multiple questions from the dataset and are essential elements of any LFS. Most data users prefer to make use of them instead of computing them on their own, to reduce the risk of error. This is a strong argument to make a case for keeping these variables in the dataset, despite them being a by-product of other original variables.
To be useful, those variables that remain in the dataset must be well documented, else they, they may be useless to or misunderstood by users.

15. Data types
Check that the data types are correct
Do not include string variables if they can be converted into numeric variables. Look at your data and check the variables’ types, particularly for those that you expect to be numeric (age, years, number of persons/employees/hours, income, purchases/expenditures, weights, and so forth). If there are numeric variables stored as string variables, your data needs cleaning.

16. Privacy protection
Datasets must not have directed identifiers
One must verify that in all data files, sensitive information or direct identifiers that could reveal the identity of the respondent directly (names, addresses, GPS coordinates, phone numbers, etc.) have been removed. Check to ensure this information is not in the dataset(s). If it is, those variables need to be removed from shared datasets.

Keep in mind that if you are preparing a dataset for public release, you need a cleaned, anonymous dataset. Removing all direct identifiers is the first key step to ensuring the anonymity of the participants. However, before you start any privacy procedures, you should always check your data.

17. Compress the variables to reduce the file size
Compress the variables consist of reducing the size of the data file without loss of precision or modifying the information that it provides. Listed below are some reasons why compressing a data set may be a useful practice for at least three reasons: First, it makes faster the process of creating backups, uploading and downloading data files from your data repository or any Survey Catalog. Second, it reduces the time that data users will need to spend working with the data. Additionally, it will make the data more accessible to the different type of users; sometimes the data size will impose restrictions on those users who lack high computational power. Third, it will help to free up disk space in the server where you store your data


## Creating a new project

A *project* consists of a survey, a census, or another type of activity that generates microdata). 

To create a new project, click on "Create new project" in the Project page and select "Microdata" as type.

![image](https://user-images.githubusercontent.com/35276300/214939118-1c290c3b-52c2-4f05-88ac-31952c04e668.png)

![image](https://user-images.githubusercontent.com/35276300/214939280-12042a85-4fc1-4f9e-acdf-261553b202cb.png)

A new project home page will be displayed. 

![image](https://user-images.githubusercontent.com/35276300/214939548-3cc62a96-c7f4-4c6b-a29d-cfd914c79abd.png)

By default, the template identified as default in the Template Manager will be used. You can select a different template by clicking on "Switch template".

![image](https://user-images.githubusercontent.com/35276300/214939822-f513121c-b659-45d1-bb7b-45a0243d471b.png)

The navigation bar on the right of the page reflects the content of the template you selected.

![image](https://user-images.githubusercontent.com/35276300/214939873-1a6bfb5f-da4f-4824-94cc-9677e2066f49.png)

You have the option to select a project thumbnail. The thumbnail can for example be the logo of your survey or census. It will be used in the NADA catalog, should you publish the metadata in NADA. The thumbnail is an image in JPG or PNG format. 

![image](https://user-images.githubusercontent.com/35276300/214940035-d99d65fe-7764-45b7-b498-d367d52c98c5.png)

To select a thumbnail, click on "Change image" and select an image file.

![image](https://user-images.githubusercontent.com/35276300/214941199-bb5fd48d-47dc-4638-a355-9e068da77279.png)

![image](https://user-images.githubusercontent.com/35276300/214941280-2493b610-1c2b-4c25-b8e5-37c07d1c6dde.png)


### Document description and Study description

Use the navigation tree to fill out information in the **Document description** and **Study description** sections.

![image](https://user-images.githubusercontent.com/35276300/214941497-887f698d-e763-48fe-8297-0e45bf6d2f73.png)

A description of the content of each element is available by clicking on the "?". If the content you enter violates a validation rule entered in the template, an error message will be displayed in red. Required elements are indicated by a red asterisk. 

![image](https://user-images.githubusercontent.com/35276300/214941921-3e765962-8573-486c-af2c-a2168e79ebf2.png)

When an element is "repeatable", an option to "Add rows" is provided.

![image](https://user-images.githubusercontent.com/35276300/214942382-a69a9dab-2410-4493-8b1e-8d2469b14868.png)

When a controlled vocabulary has been entered in the template, a drop down menu will appear.

![image](https://user-images.githubusercontent.com/35276300/214942534-d47df5a3-93f0-4d61-b956-46bbc89f0632.png)
Try and provide as much and as relevant information in all relevant metadata elements. 

### Importing data files

If you have data files available in CSV, Stata (.dta), or SPSS (.sav) format, you can import the data to automatically populate some of the content of the *File description* section and much of the content of the *Variable description* section. The Metadata Editor relies on the open source R software and on the Haven library to import (and re-export) data files. When a data file is imported, the application will:
- Generate the list of variables for each data file
- Import the variable and value labels from the data files, if available
- Generate summary statistics that may be saved as metadata.

To import your data files, select "Data files" in the navigation bar, and click on "Import files".

![image](https://user-images.githubusercontent.com/35276300/214943689-7c608a52-777c-41c2-9662-d1c66797e370.png)

Select the data files you want to import, and click "Import files".

![image](https://user-images.githubusercontent.com/35276300/214944454-86919d0a-e0ae-40dd-a537-a94122afa4b3.png)
![image](https://user-images.githubusercontent.com/35276300/214952350-92cf2cdd-371c-4a4e-8fda-08653a4cb5b3.png)

Success
![image](https://user-images.githubusercontent.com/35276300/214952439-00f9a702-16a9-4b82-8bb1-690d68aaf966.png)

### Creating data files


### File description

A brief description of each file can be entered. Select the filename in the navigation bar, and fill out the form. Then save.
![image](https://user-images.githubusercontent.com/35276300/214966052-85ffabc6-7d04-4ca3-b3c0-800c44499b68.png)

### Data

Data page:
![image](https://user-images.githubusercontent.com/35276300/214965888-3af24fa2-8fa4-4d3a-a225-bc90f62130a8.png)

### Variable description and statistics

For each data file, the Variables section shows a variable list and metadata. Additional metadata can be entered for each variable.
![image](https://user-images.githubusercontent.com/35276300/214965734-f5d357e0-af79-4d08-9f12-1025adccf605.png)
What has been imported from the data files:
- List of variables with name, label and type ("Variables" block).
- Value labels for categorical variables ("Categories" block).
- Variable information: type, decimals, format, missing values

Documentation:
- Statistics
- Weights
- Documentation
![image](https://user-images.githubusercontent.com/35276300/214967059-352ab9cf-4dde-4867-85fe-dd40ba0bdbc6.png)

In the variable page: Description
- Ctrl and Shift keys
- Spread metadata
- Import metadata


- JSON

Variable labels can be added or changed directly in the variable list table.
   - Spread metadata
   - 

Categories can be added or edited. 
   - Generate categories from statistics button:
   - Copy / paste
   - Store in repository


Weights
Keys

Summary statistics

### Variable groups

### Tags

### External resources

![image](https://user-images.githubusercontent.com/35276300/214945692-0e3a37e5-14b0-495c-8eeb-351d305fb185.png)


### Saving and exporting metadata


### Exporting data


### Diagnostics


## Editing an existing project

Open and edit
Replacing data files



